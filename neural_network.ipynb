{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "neural_network.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLNhl2VvGgLX"
      },
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyRrRSgGG3Xs",
        "outputId": "84b5cea1-9083-4f7f-e0a5-bd18c182f41e"
      },
      "source": [
        "check = ((Y_train == 1) | (Y_train == 0)) \n",
        "print(check.shape)\n",
        "X_train = X_train[check]\n",
        "Y_train = Y_train[check]\n",
        "#[0 1 2] -> [1 0 0] [0 1 0] [0 0 1]\n",
        "#vector zeros co do dai bang so class -> vector độ dài C -> [0, 0, 0..., 0]-> dữ liệu đc gán nhãn j -> y[j] = 1\n",
        "from keras.utils import np_utils\n",
        "Y_train = np_utils.to_categorical(Y_train)\n",
        "print(Y_train.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000,)\n",
            "(12665, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3ssQYwLId-O",
        "outputId": "9bc02903-cca7-4fd4-a82f-cdb7a8742906"
      },
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1] * X_train.shape[2])/255.\n",
        "X_train = np.hstack((X_train, np.zeros((X_train.shape[0], 1))))\n",
        "print(X_train.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(12665, 785)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-7EQp_8JcLA",
        "outputId": "2735b855-98f8-4cf0-81a7-9d83cb3d0bea"
      },
      "source": [
        "def softmax(Z):\n",
        "  #print(Z.shape)\n",
        "  #e^a/(e^a + e^b + e^c) = e^(a - c) / e^(a-c) + e^(b-c) + 1\n",
        "  max_Z = np.max(Z, axis = 1)\n",
        "  #Z co shape (N, classes)\n",
        "  #print(max_Z.shape)\n",
        "  Z_t = Z.T\n",
        "  Z_t = Z_t - max_Z\n",
        "  new_Z = Z_t.T\n",
        "  new_Z = np.exp(new_Z)\n",
        "  sum_eZT = np.sum(new_Z, axis = 1).reshape(-1)\n",
        "  ez_T = new_Z.T\n",
        "  ez_T /= sum_eZT\n",
        "  ez = ez_T.T\n",
        "  return ez\n",
        "x = np.array([[1, 2, 3],\n",
        "              [3, 4, 5],\n",
        "              [4, 5, 6],\n",
        "              ])\n",
        "#[3, 5, 6]\n",
        "#[[1, 3, 4], [2, 4, 5] [3, 5, 6]] - [3, 5, 6] = [[1, 3, 4]- [3, 5, 6], [2, 4, 5]- [3, 5, 6], [3, 5, 6] - [3, 5, 6]] \n",
        "print(np.sum(softmax(x),axis = 1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1. 1. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KUq-KbzQ0K9",
        "outputId": "88820082-5e82-4d3c-b245-0552130c96bc"
      },
      "source": [
        "def relu(Z):\n",
        "  new_Z = Z\n",
        "  new_Z[new_Z < 0] = 0.\n",
        "  return new_Z\n",
        "def sigmoid(Z):\n",
        "  return 1/(np.exp(-Z) + 1)\n",
        "def activation(Z, type):\n",
        "  if type == \"relu\":\n",
        "    return relu(Z)\n",
        "  if type == \"softmax\":\n",
        "    return softmax(Z)\n",
        "  if type == \"sigmoid\":\n",
        "    return sigmoid(Z)\n",
        "  return Z\n",
        "def gradient(Z, type):\n",
        "  if type == \"relu\":\n",
        "    new_Z = np.ones(Z.shape)\n",
        "    new_Z[new_Z < 0] = 0.\n",
        "    return new_Z\n",
        "  if type == \"softmax\":\n",
        "    return softmax(Z) * (1 - softmax(Z))\n",
        "  if type == \"sigmoid\":\n",
        "    return sigmoid(Z) * (1 -sigmoid(Z))\n",
        "  return np.ones(Z.shape)\n",
        "def crossentropy_loss(Z, Y):\n",
        "  return -np.sum(np.sum(Y * np.log(Z), axis = 1),axis = 0)/Y.shape[0]\n",
        "class NeuralNetwork():\n",
        "  def __init__(self, layers_node, layers_activation, input, output):\n",
        "    #(200, 10)-> 128 -> [200, 128]-> (10, 128)\n",
        "    #layers_information : [(128, \"\"), (128, \"softmax\")]\n",
        "    self.layers_activation = layers_activation\n",
        "    self.layers_node = layers_node\n",
        "    #np.random.rand(self.layers_node[index - 1], layer_node)\n",
        "    #np.full((input.shape[1], layer_node), 0.1)\n",
        "    #np.full((self.layers_node[index - 1], layer_node)\n",
        "    self.weights = [(np.random.rand(input.shape[1], layer_node) if index == 0 else np.random.rand(self.layers_node[index - 1], layer_node)) \n",
        "                      for (index, layer_node) in enumerate(self.layers_node)]\n",
        "    self.input = input\n",
        "    self.output = output\n",
        "  def feed_forward(self, input):\n",
        "    output = input.copy()\n",
        "    Z = []\n",
        "    A = []\n",
        "    A.append(input)\n",
        "    for index, weight in enumerate(self.weights):\n",
        "      output = output.dot(weight)\n",
        "      Z.append(output)\n",
        "      output = activation(output, self.layers_activation[index])\n",
        "      A.append(output)\n",
        "    return Z, A\n",
        "  def backpropagation(self, input, output, lr):\n",
        "      Z, A = self.feed_forward(input)\n",
        "      #print(Z[0].shape)\n",
        "      #print(A[0].shape)\n",
        "      E = []\n",
        "      for i in range(len(self.weights) - 1, -1, -1):\n",
        "        if i == len(self.weights) - 1:\n",
        "          E.append(A[i + 1] - output)\n",
        "        else:\n",
        "          E.append((E[-1].dot(self.weights[i + 1].T))* gradient(Z[i], self.layers_activation[i]))\n",
        "        #print(A[i - 1].shape)\n",
        "        #print(E[-1].shape)\n",
        "        self.weights[i] -= lr * (A[i].T.dot(E[-1]))\n",
        "      return Z, A\n",
        "  def training(self, lr, number_iterations, batch_size):\n",
        "    for epoch in range(number_iterations):\n",
        "      random = np.random.permutation(int(X_train.shape[0]/batch_size))\n",
        "      sum_loss = 0\n",
        "      for j in random:\n",
        "        Xj = self.input[j * batch_size : (j + 1) * batch_size]\n",
        "        Yj = self.output[j * batch_size : (j + 1) * batch_size]\n",
        "        Z, A = self.backpropagation(Xj, Yj, lr)\n",
        "        sum_loss += crossentropy_loss(A[-1], Yj)\n",
        "      if epoch % 10 == 0:\n",
        "        print(sum_loss / random.shape[0])\n",
        "model = NeuralNetwork([128, 128, 2], [\"sigmoid\", \"sigmoid\", \"softmax\"], X_train, Y_train)\n",
        "model.training(0.00001, 1000, 32)\n",
        "#4600 = N \n",
        "#200, 100\n",
        "#4600 , 100\n",
        "#100 , 20\n",
        "#4600, 20 * 20, 100 = 4600, 100"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "379.6854150211065\n",
            "273.0274957441584\n",
            "272.9995319812653\n",
            "273.00175839022353\n",
            "272.9915426073169\n",
            "273.03208028480105\n",
            "273.0246217582235\n",
            "273.02017126889353\n",
            "273.03792874008394\n",
            "273.0258544851149\n",
            "273.0388533415231\n",
            "273.0223426737837\n",
            "273.0368668373593\n",
            "273.03749909339786\n",
            "273.00941789316875\n",
            "273.04028324470596\n",
            "273.0093179267153\n",
            "273.0022328880753\n",
            "273.00167179475477\n",
            "273.0219932833992\n",
            "273.0136057145978\n",
            "273.044687938287\n",
            "272.98353895425873\n",
            "272.98592974357774\n",
            "273.00867196763494\n",
            "273.0280072310538\n",
            "273.0137883043949\n",
            "272.9223299864345\n",
            "273.0251138403501\n",
            "273.02192805219784\n",
            "273.0065224219725\n",
            "273.03220701805236\n",
            "273.03297906207337\n",
            "272.9936111695796\n",
            "272.9918986407755\n",
            "273.0556562469772\n",
            "273.01478842892\n",
            "273.0446936212702\n",
            "273.0276201945138\n",
            "273.02853995124946\n",
            "273.0389412520207\n",
            "273.01837978196096\n",
            "272.9784949305759\n",
            "273.02757671549773\n",
            "273.0202313298879\n",
            "273.0378533947048\n",
            "272.983708404383\n",
            "272.99336251631894\n",
            "273.0231269937286\n",
            "273.03469259804797\n",
            "272.98659217071406\n",
            "273.0127594455659\n",
            "273.00827538809415\n",
            "273.02656702354835\n",
            "273.0414138780468\n",
            "273.0389407974509\n",
            "273.0337426578081\n",
            "273.01392647009556\n",
            "273.0300167887663\n",
            "273.0310239444562\n",
            "272.99967793808014\n",
            "273.03286840683563\n",
            "273.0328659443541\n",
            "273.0315303192967\n",
            "273.0179909346256\n",
            "273.0464738233396\n",
            "273.02857556923107\n",
            "273.03194664045606\n",
            "273.02742151362156\n",
            "273.0337357391598\n",
            "273.032884941073\n",
            "273.01823525500265\n",
            "273.0132626104893\n",
            "272.996904501166\n",
            "273.0079517143248\n",
            "273.0388897231546\n",
            "272.9789226385878\n",
            "273.0288420179216\n",
            "273.0368928342306\n",
            "273.0316863703762\n",
            "273.013120113013\n",
            "273.03693246743404\n",
            "273.02367452161803\n",
            "273.00198230897576\n",
            "273.00860046920957\n",
            "272.9454303858116\n",
            "273.03639548110505\n",
            "273.01585422926973\n",
            "272.9851031225865\n",
            "272.9996183110011\n",
            "272.97339122972767\n",
            "272.9685355996631\n",
            "273.02132986976915\n",
            "272.98649359099227\n",
            "273.03677644904917\n",
            "273.01864214761434\n",
            "273.0276196201248\n",
            "273.0040695087994\n",
            "273.02936161721664\n",
            "273.0192735454254\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "tLd3adpqSuJ4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}